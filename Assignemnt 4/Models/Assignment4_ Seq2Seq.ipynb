{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment4_2.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "0a_h3W1zKA0G",
        "colab_type": "code",
        "outputId": "fc879e64-d7ab-4721-ce12-64497bff907f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/avikdelta/parse_seq2seq.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'parse_seq2seq'...\n",
            "remote: Enumerating objects: 45, done.\u001b[K\n",
            "remote: Total 45 (delta 0), reused 0 (delta 0), pack-reused 45\u001b[K\n",
            "Unpacking objects: 100% (45/45), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hxiWgj5_KB7k",
        "colab_type": "code",
        "outputId": "124079a4-1501-49bf-a9c0-ecca35486937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "cd parse_seq2seq/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/parse_seq2seq\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FNJr9vaF5SKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# author: Avik Ray (avik.r@samsung.com) \n",
        "#\n",
        "# script modified from tensorflow ENG->FR machine translation using \n",
        "# sequence-to-sequence tutorial code by The TensorFlow Authors\n",
        "# \n",
        "# ========================================================================\n",
        "\"\"\"Binary for training semantic parser model and decoding from them.\n",
        "example usage: \n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
        "import tensorflow as tf\n",
        "\n",
        "import data_utils\n",
        "import seq2seq_model\n",
        "from accuracy import *\n",
        "\n",
        "\n",
        "tf.app.flags.DEFINE_float(\"learning_rate\", 0.01, \"Learning rate.\")\n",
        "tf.app.flags.DEFINE_float(\"learning_rate_decay_factor\", 0.98,\n",
        "                          \"Learning rate decays by this much.\")\n",
        "tf.app.flags.DEFINE_float(\"max_gradient_norm\", 5.0,\n",
        "                          \"Clip gradients to this norm.\")\n",
        "tf.app.flags.DEFINE_float(\"dropout\", 0.5,\n",
        "                          \"dropout rate.\")\n",
        "tf.app.flags.DEFINE_integer(\"batch_size\", 20,\n",
        "                            \"Batch size to use during training.\")\n",
        "tf.app.flags.DEFINE_integer(\"size\", 200, \"Size of each model layer.\")\n",
        "tf.app.flags.DEFINE_integer(\"num_layers\", 1, \"Number of layers in the model.\")\n",
        "tf.app.flags.DEFINE_integer(\"from_vocab_size\", 2000, \"Max natural language input vocabulary size.\")\n",
        "tf.app.flags.DEFINE_integer(\"to_vocab_size\", 2000, \"Max logical form vocabulary size.\")\n",
        "tf.app.flags.DEFINE_string(\"data_dir\", \"/tmp\", \"Data directory\")\n",
        "tf.app.flags.DEFINE_string(\"train_dir\", \"/tmp\", \"Training/Checkpoint directory.\")\n",
        "tf.app.flags.DEFINE_string(\"train_file\", None, \"Training data.\")\n",
        "tf.app.flags.DEFINE_string(\"dev_file\", None, \"Validation data.\")\n",
        "tf.app.flags.DEFINE_string(\"test_file\", None, \"Test data.\")\n",
        "tf.app.flags.DEFINE_integer(\"max_train_data_size\", 0,\n",
        "                            \"Limit on the size of training data (0: no limit).\")\n",
        "tf.app.flags.DEFINE_integer(\"display_every\", 100,\n",
        "                            \"How many training steps/batches to do per display output.\")\n",
        "tf.app.flags.DEFINE_integer(\"max_epoch\", 90,\n",
        "                            \"maximum number of epochs to train.\")\n",
        "tf.app.flags.DEFINE_integer(\"eval_per_epoch\", 10,\n",
        "                            \"evaluation frequency\")\n",
        "tf.app.flags.DEFINE_boolean(\"decode\", False,\n",
        "                            \"Set to True for interactive decoding.\")\n",
        "tf.app.flags.DEFINE_boolean(\"test\", False,\n",
        "                            \"Set to True for computing test accuracy.\")\n",
        "tf.app.flags.DEFINE_boolean(\"valid\", True,\n",
        "                            \"True = Use valid set for evaluation. False = Use test set for evaluation.\")\n",
        "tf.app.flags.DEFINE_boolean(\"display\", False,\n",
        "                            \"Display inference/test output.\")\n",
        "tf.app.flags.DEFINE_boolean(\"self_test\", False,\n",
        "                            \"Run a self-test if this is set to True.\")\n",
        "tf.app.flags.DEFINE_boolean(\"use_fp16\", False,\n",
        "                            \"Train using fp16 instead of fp32.\")\n",
        "\n",
        "\n",
        "\n",
        "FLAGS = tf.app.flags.FLAGS\n",
        "\n",
        "# We use a number of buckets and pad to the closest one for efficiency.\n",
        "# See seq2seq_model.Seq2SeqModel for details of how they work.\n",
        "#_buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]\n",
        "_buckets = [(50, 130)]\n",
        "\n",
        "\n",
        "def read_data(source_path, target_path, max_size=None):\n",
        "  \"\"\"Read data from source and target files and put into buckets.\n",
        "  Args:\n",
        "    source_path: path to the files with token-ids for the source language.\n",
        "    target_path: path to the file with token-ids for the target language;\n",
        "      it must be aligned with the source file: n-th line contains the desired\n",
        "      output for n-th line from the source_path.\n",
        "    max_size: maximum number of lines to read, all other will be ignored;\n",
        "      if 0 or None, data files will be read completely (no limit).\n",
        "  Returns:\n",
        "    data_set: a list of length len(_buckets); data_set[n] contains a list of\n",
        "      (source, target) pairs read from the provided data files that fit\n",
        "      into the n-th bucket, i.e., such that len(source) < _buckets[n][0] and\n",
        "      len(target) < _buckets[n][1]; source and target are lists of token-ids.\n",
        "  \"\"\"\n",
        "  data_set = [[] for _ in _buckets]\n",
        "  with tf.gfile.GFile(source_path, mode=\"r\") as source_file:\n",
        "    with tf.gfile.GFile(target_path, mode=\"r\") as target_file:\n",
        "      source, target = source_file.readline(), target_file.readline()\n",
        "      counter = 0\n",
        "      while source and target and (not max_size or counter < max_size):\n",
        "        counter += 1\n",
        "        if counter % 100000 == 0:\n",
        "          print(\"  reading data line %d\" % counter)\n",
        "          sys.stdout.flush()\n",
        "        source_ids = [int(x) for x in source.split()]\n",
        "        target_ids = [int(x) for x in target.split()]\n",
        "        target_ids.append(data_utils.EOS_ID)\n",
        "        for bucket_id, (source_size, target_size) in enumerate(_buckets):\n",
        "          if len(source_ids) < source_size and len(target_ids) < target_size:\n",
        "            data_set[bucket_id].append([source_ids, target_ids])\n",
        "            break\n",
        "        source, target = source_file.readline(), target_file.readline()\n",
        "  return data_set\n",
        "\n",
        "\n",
        "def create_model(session, forward_only, use_dropout):\n",
        "  \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
        "  dtype = tf.float16 if FLAGS.use_fp16 else tf.float32\n",
        "  model = seq2seq_model.Seq2SeqModel(\n",
        "      FLAGS.from_vocab_size,\n",
        "      FLAGS.to_vocab_size,\n",
        "      _buckets,\n",
        "      FLAGS.size,\n",
        "      FLAGS.num_layers,\n",
        "      FLAGS.max_gradient_norm,\n",
        "      FLAGS.batch_size,\n",
        "      FLAGS.learning_rate,\n",
        "      FLAGS.learning_rate_decay_factor,\n",
        "      forward_only=forward_only,\n",
        "      dropout=FLAGS.dropout,\n",
        "      use_dropout=use_dropout,\n",
        "      dtype=dtype)\n",
        "  ckpt = tf.train.get_checkpoint_state(FLAGS.train_dir)\n",
        "  if FLAGS.decode or FLAGS.test:\n",
        "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
        "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
        "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
        "    else:\n",
        "        print(\"Created model with fresh parameters.\")\n",
        "        session.run(tf.global_variables_initializer())\n",
        "  else:\n",
        "    print(\"Created model with fresh parameters.\")\n",
        "    session.run(tf.global_variables_initializer())\n",
        "  return model\n",
        "\n",
        "#-----------------------------------------------------\n",
        "# creates batches from the training data for training\n",
        "#-----------------------------------------------------\n",
        "def create_batches(data):\n",
        "    print(\"generating batches...\")\n",
        "    batches = [[] for _ in _buckets]\n",
        "    for bucket_id in xrange(len(_buckets)):\n",
        "        data_bucket = data[bucket_id]\n",
        "        encoder_size, decoder_size = _buckets[bucket_id]\n",
        "        \n",
        "        # shuffle the data\n",
        "        data_permute = np.random.permutation(len(data_bucket))\n",
        "        \n",
        "        num_batches = math.ceil(len(data_bucket)/FLAGS.batch_size)\n",
        "        for b_idx in xrange(num_batches):\n",
        "            encoder_inputs, decoder_inputs = [], []\n",
        "            for i in xrange(FLAGS.batch_size):\n",
        "                data_idx = data_permute[(b_idx*FLAGS.batch_size+i) % len(data_bucket)]\n",
        "                encoder_input, decoder_input = data_bucket[data_idx]\n",
        "\n",
        "                # Encoder inputs are padded and then reversed.\n",
        "                encoder_pad = [data_utils.PAD_ID] * (encoder_size - len(encoder_input))\n",
        "                encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
        "\n",
        "                # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
        "                decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
        "                decoder_inputs.append([data_utils.GO_ID] + decoder_input + [data_utils.PAD_ID] * decoder_pad_size)\n",
        "                \n",
        "            # Now we create batch-major vectors from the data selected above.\n",
        "            batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
        "\n",
        "            # Batch encoder inputs are just re-indexed encoder_inputs.\n",
        "            for length_idx in xrange(encoder_size):\n",
        "                batch_encoder_inputs.append(np.array([encoder_inputs[batch_idx][length_idx] \n",
        "                                            for batch_idx in xrange(FLAGS.batch_size)], dtype=np.int32))\n",
        "\n",
        "            # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
        "            for length_idx in xrange(decoder_size):\n",
        "                batch_decoder_inputs.append(np.array([decoder_inputs[batch_idx][length_idx]\n",
        "                                            for batch_idx in xrange(FLAGS.batch_size)], dtype=np.int32))\n",
        "\n",
        "                # Create target_weights to be 0 for targets that are padding.\n",
        "                batch_weight = np.ones(FLAGS.batch_size, dtype=np.float32)\n",
        "                for batch_idx in xrange(FLAGS.batch_size):\n",
        "                    # We set weight to 0 if the corresponding target is a PAD symbol.\n",
        "                    # The corresponding target is decoder_input shifted by 1 forward.\n",
        "                    if length_idx < decoder_size - 1:\n",
        "                        target = decoder_inputs[batch_idx][length_idx + 1]\n",
        "                    if length_idx == decoder_size - 1 or target == data_utils.PAD_ID:\n",
        "                        batch_weight[batch_idx] = 0.0\n",
        "                        \n",
        "                batch_weights.append(batch_weight)\n",
        "                \n",
        "            batches[bucket_id].append((batch_encoder_inputs, batch_decoder_inputs, batch_weights))\n",
        "            \n",
        "    return batches\n",
        "    \n",
        "#-----------------------------------------------------\n",
        "# main training function\n",
        "#-----------------------------------------------------\n",
        "def train():\n",
        "  \"\"\"Train a query->logical form semantic parser model\"\"\"\n",
        "  from_train = None\n",
        "  to_train = None\n",
        "  from_dev = None\n",
        "  to_dev = None\n",
        "  \n",
        "  # process training data\n",
        "  from_train_data = os.path.join(FLAGS.data_dir,\"train_q.txt\")\n",
        "  to_train_data = os.path.join(FLAGS.data_dir,\"train_f.txt\")\n",
        "  \n",
        "  if FLAGS.train_file:\n",
        "    print(\"using training files\",from_train_data,\"and\",to_train_data)\n",
        "    from_dev_data = from_train_data\n",
        "    to_dev_data = to_train_data\n",
        "    data_utils.splitToFrom(FLAGS.data_dir,FLAGS.train_file,\"train\") # split to-from data\n",
        "    if FLAGS.dev_file:\n",
        "      data_utils.splitToFrom(FLAGS.data_dir,FLAGS.dev_file,\"valid\") # split to-from data\n",
        "      from_dev_data = os.path.join(FLAGS.data_dir,\"valid_q.txt\")\n",
        "      to_dev_data = os.path.join(FLAGS.data_dir,\"valid_f.txt\")\n",
        "      print(\"using validation files\",from_dev_data,\"and\",to_dev_data,\"for validation\")\n",
        "    elif FLAGS.test_file:\n",
        "      data_utils.splitToFrom(FLAGS.data_dir,FLAGS.test_file,\"test\") # split to-from data\n",
        "      from_dev_data = os.path.join(FLAGS.data_dir,\"test_q.txt\")\n",
        "      to_dev_data = os.path.join(FLAGS.data_dir,\"test_f.txt\")\n",
        "      print(\"using test files\",from_dev_data,\"and\",to_dev_data,\"for validation\")\n",
        "    else:\n",
        "      print(\"using train files\",from_dev_data,\"and\",to_dev_data,\"for validation\")\n",
        "    \n",
        "  \n",
        "    from_train, to_train, from_dev, to_dev, _, _ = data_utils.prepare_data(\n",
        "        FLAGS.data_dir,\n",
        "        from_train_data,\n",
        "        to_train_data,\n",
        "        from_dev_data,\n",
        "        to_dev_data,\n",
        "        FLAGS.from_vocab_size,\n",
        "        FLAGS.to_vocab_size)\n",
        "  else:\n",
        "      # Prepare data.\n",
        "      print(\"Preparing data in %s\" % FLAGS.data_dir)\n",
        "      \n",
        "  print(\"data preparation complete!\")\n",
        "  \n",
        "  config_ = tf.ConfigProto()\n",
        "  config_.gpu_options.allow_growth = True\n",
        "  config_.allow_soft_placement = True\n",
        "  with tf.Session(config=config_) as sess:\n",
        "    # Create model.\n",
        "    print(\"Creating %d layers of %d units.\" % (FLAGS.num_layers, FLAGS.size))\n",
        "    model = create_model(sess, False, True)\n",
        "\n",
        "    # Read data into buckets and compute their sizes.\n",
        "    print (\"Reading development and training data (limit: %d).\"\n",
        "           % FLAGS.max_train_data_size)\n",
        "    dev_set = read_data(from_dev, to_dev)\n",
        "    train_set = read_data(from_train, to_train, FLAGS.max_train_data_size)\n",
        "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(_buckets))]\n",
        "    print(\"train_bucket_sizes =\",train_bucket_sizes)\n",
        "    train_total_size = float(sum(train_bucket_sizes))\n",
        "    print(\"train_total_size =\",train_total_size)\n",
        "                           \n",
        "    # create data batches for training\n",
        "    all_train_batches = create_batches(train_set)\n",
        "\n",
        "    # This is the training loop.\n",
        "    step_time, loss = 0.0, 0.0\n",
        "    current_step = 0\n",
        "    epoch_count = 1\n",
        "    epoch_size = math.ceil(train_total_size/model.batch_size)\n",
        "    previous_losses = []\n",
        "    print(\"------------------------------------------\")\n",
        "    print(\"Starting training: max epochs =\",FLAGS.max_epoch,\"; epoch size =\",epoch_size,\"; train_total_size =\",train_total_size)\n",
        "    print(\"------------------------------------------\")\n",
        "    \n",
        "    # generate random permutation of batch numbers for epoch\n",
        "    batch_permutations = [np.random.permutation(len(all_train_batches[bkt_idx])) for bkt_idx in xrange(len(_buckets))]\n",
        "    batch_pidx = [0 for bkt_idx in xrange(len(_buckets))]\n",
        "    \n",
        "    while True:\n",
        "      \n",
        "      bucket_id = 0 # we just have one bucket\n",
        "\n",
        "      # Get a batch and make a step.\n",
        "        \n",
        "      # select batch for this iteration\n",
        "      bidx = batch_permutations[bucket_id][batch_pidx[bucket_id]]\n",
        "      encoder_inputs, decoder_inputs, target_weights = all_train_batches[bucket_id][bidx]\n",
        "      batch_pidx[bucket_id] = (batch_pidx[bucket_id] + 1) % len(all_train_batches[bucket_id])\n",
        "      \n",
        "      # execute gradient descent step\n",
        "      start_time = time.time()\n",
        "      _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                   target_weights, bucket_id, False)\n",
        "      step_time += (time.time() - start_time) / FLAGS.display_every\n",
        "      loss += step_loss / FLAGS.display_every\n",
        "      current_step += 1\n",
        "\n",
        "      # Once in a while, we save checkpoint, print statistics, and run evals.\n",
        "      if current_step % FLAGS.display_every == 0:\n",
        "        # Print statistics for the previous epoch.\n",
        "        print (\"epoch %d/%d step %d learning rate %.6f time/batch %.2f loss \"\n",
        "               \"%2.5f\" % (epoch_count,FLAGS.max_epoch,model.global_step.eval(), model.learning_rate.eval(),\n",
        "                         step_time, loss))\n",
        "          \n",
        "        previous_losses.append(loss)\n",
        "        \n",
        "        # zero timer and loss.\n",
        "        step_time, loss = 0.0, 0.0\n",
        "        sys.stdout.flush()\n",
        "        \n",
        "      # all epoch updates\n",
        "      if current_step%epoch_size==0:\n",
        "        epoch_count += 1\n",
        "        \n",
        "        # generate random permutation of batch numbers for epoch\n",
        "        batch_permutations = [np.random.permutation(len(all_train_batches[bkt_idx])) for bkt_idx in xrange(len(_buckets))]\n",
        "        batch_pidx = [0 for bkt_idx in xrange(len(_buckets))]\n",
        "    \n",
        "        # Decrease learning rate\n",
        "        if epoch_count>5:\n",
        "            sess.run(model.learning_rate_decay_op)\n",
        "        \n",
        "        # Run evals on development set and print their perplexity.\n",
        "        if (epoch_count-1)%FLAGS.eval_per_epoch==0 or epoch_count>FLAGS.max_epoch:\n",
        "            \n",
        "            for bucket_id in xrange(len(_buckets)):\n",
        "                if len(dev_set[bucket_id]) == 0:\n",
        "                    print(\"  eval: empty bucket %d\" % (bucket_id))\n",
        "                    continue\n",
        "            \n",
        "                encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "                    dev_set, bucket_id)\n",
        "                _, eval_loss, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "                eval_ppx = math.exp(float(eval_loss)) if eval_loss < 300 else float(\"inf\")\n",
        "                print(\"  epoch %d eval: bucket %d perplexity %.2f loss %2.5f\" % (epoch_count-1,bucket_id, eval_ppx, eval_loss))\n",
        "        \n",
        "        # save model at the end of last training epoch\n",
        "        if epoch_count > FLAGS.max_epoch:\n",
        "            print(\"Max epoch reached!\")\n",
        "            # Save checkpoint \n",
        "            checkpoint_path = os.path.join(FLAGS.train_dir, \"parse.ckpt\")\n",
        "            model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
        "            print(\"Model saved.\")\n",
        "            break\n",
        "            \n",
        "    print(\"Training complete!\")\n",
        "    return\n",
        "\n",
        "#-----------------------------------------------------\n",
        "# function computing test accuracy\n",
        "#-----------------------------------------------------\n",
        "def test_accuracy(from_data,to_data):\n",
        "\n",
        "    config_ = tf.ConfigProto()\n",
        "    config_.gpu_options.allow_growth = True\n",
        "    config_.allow_soft_placement = True\n",
        "    with tf.Session(config=config_) as sess:\n",
        "        # Create model and load parameters.\n",
        "        model = create_model(sess, True, False)\n",
        "        model.batch_size = 1  # We decode one sentence at a time.\n",
        "\n",
        "        print(\"loading data...\")\n",
        "        # Load vocabularies.\n",
        "        from_vocab_path = os.path.join(FLAGS.data_dir,\"vocab%d.from\" % FLAGS.from_vocab_size)\n",
        "        to_vocab_path = os.path.join(FLAGS.data_dir,\"vocab%d.to\" % FLAGS.to_vocab_size)\n",
        "        from_vocab, _ = data_utils.initialize_vocabulary(from_vocab_path)\n",
        "        to_vocab, rev_to_vocab = data_utils.initialize_vocabulary(to_vocab_path)\n",
        "\n",
        "        # read test data\n",
        "        test_data = data_utils.tokenize_dataset(from_data,to_data,from_vocab,to_vocab)\n",
        "        print(\"data loaded. computing accuracy...\")\n",
        "        val_acc = 0\n",
        "        comm_dict = init_comm_dict(to_vocab)\n",
        "        rev_to_vocab_dict = reverseDict(to_vocab)\n",
        "        \n",
        "        for data in test_data:\n",
        "\n",
        "            from_token_ids = data[0]\n",
        "            to_token_ids = data[1]\n",
        "      \n",
        "            # Which bucket does it belong to?\n",
        "            bucket_id = len(_buckets) - 1\n",
        "            for i, bucket in enumerate(_buckets):\n",
        "                if bucket[0] >= len(from_token_ids):\n",
        "                    bucket_id = i\n",
        "                    break\n",
        "                else:\n",
        "                    logging.warning(\"Sentence truncated: %s\", sentence)\n",
        "\n",
        "            # Get a 1-element batch to feed the sentence to the model.\n",
        "            encoder_inputs, decoder_inputs, target_weights = model.get_batch({bucket_id: [(from_token_ids, [])]}, bucket_id)\n",
        "            # Get output logits for the sentence.\n",
        "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
        "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "            # If there is an EOS symbol in outputs, cut them at that point.\n",
        "            if data_utils.EOS_ID in outputs:\n",
        "                outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
        "            \n",
        "            outputs = post_process(outputs,to_vocab)\n",
        "            \n",
        "            if compute_tree_accuracy(outputs,to_token_ids,to_vocab,rev_to_vocab_dict,comm_dict,FLAGS.display):\n",
        "                val_acc+= 1\n",
        "                \n",
        "            \n",
        "        val_acc = val_acc/float(len(test_data))\n",
        "        \n",
        "    return val_acc\n",
        "            \n",
        "\n",
        "#-----------------------------------------------------\n",
        "# function for interactive decoding\n",
        "#-----------------------------------------------------\n",
        "def decode():\n",
        "  config_ = tf.ConfigProto()\n",
        "  config_.gpu_options.allow_growth = True\n",
        "  config_.allow_soft_placement = True\n",
        "  with tf.Session(config=config_) as sess:\n",
        "    # Create model and load parameters.\n",
        "    model = create_model(sess, True, False)\n",
        "    model.batch_size = 1  # We decode one sentence at a time.\n",
        "\n",
        "    # Load vocabularies.\n",
        "    from_vocab_path = os.path.join(FLAGS.data_dir,\n",
        "                                 \"vocab%d.from\" % FLAGS.from_vocab_size)\n",
        "    to_vocab_path = os.path.join(FLAGS.data_dir,\n",
        "                                 \"vocab%d.to\" % FLAGS.to_vocab_size)\n",
        "    from_vocab, _ = data_utils.initialize_vocabulary(from_vocab_path)\n",
        "    _, rev_to_vocab = data_utils.initialize_vocabulary(to_vocab_path)\n",
        "\n",
        "    # Decode from standard input.\n",
        "    sys.stdout.write(\"> \")\n",
        "    sys.stdout.flush()\n",
        "    sentence = sys.stdin.readline()\n",
        "    while sentence:\n",
        "      # Get token-ids for the input sentence.\n",
        "      token_ids = data_utils.sentence_to_token_ids(sentence, from_vocab)\n",
        "      \n",
        "      print(token_ids)\n",
        "      \n",
        "      # Which bucket does it belong to?\n",
        "      bucket_id = len(_buckets) - 1\n",
        "      for i, bucket in enumerate(_buckets):\n",
        "        if bucket[0] >= len(token_ids):\n",
        "          bucket_id = i\n",
        "          break\n",
        "      else:\n",
        "        logging.warning(\"Sentence truncated: %s\", sentence)\n",
        "\n",
        "      # Get a 1-element batch to feed the sentence to the model.\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          {bucket_id: [(token_ids, [])]}, bucket_id)\n",
        "      # Get output logits for the sentence.\n",
        "      _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,\n",
        "                                       target_weights, bucket_id, True)\n",
        "      # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
        "      outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
        "      # If there is an EOS symbol in outputs, cut them at that point.\n",
        "      if data_utils.EOS_ID in outputs:\n",
        "        outputs = outputs[:outputs.index(data_utils.EOS_ID)]\n",
        "      # Print out logical form corresponding to outputs.\n",
        "      print(\" \".join([tf.compat.as_str(rev_to_vocab[output]) for output in outputs]))\n",
        "      print(\"> \", end=\"\")\n",
        "      sys.stdout.flush()\n",
        "      sentence = sys.stdin.readline()\n",
        "\n",
        "\n",
        "def self_test():\n",
        "  \"\"\"Test the translation model.\"\"\"\n",
        "  config_ = tf.ConfigProto()\n",
        "  config_.gpu_options.allow_growth = True\n",
        "  config_.allow_soft_placement = True\n",
        "  with tf.Session(config=config_) as sess:\n",
        "    print(\"Self-test for neural translation model.\")\n",
        "    # Create model with vocabularies of 10, 2 small buckets, 2 layers of 32.\n",
        "    model = seq2seq_model.Seq2SeqModel(10, 10, [(3, 3), (6, 6)], 32, 2,\n",
        "                                       5.0, 32, 0.3, 0.99, num_samples=8)\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    # Fake data set for both the (3, 3) and (6, 6) bucket.\n",
        "    data_set = ([([1, 1], [2, 2]), ([3, 3], [4]), ([5], [6])],\n",
        "                [([1, 1, 1, 1, 1], [2, 2, 2, 2, 2]), ([3, 3, 3], [5, 6])])\n",
        "    for _ in xrange(5):  # Train the fake model for 5 steps.\n",
        "      bucket_id = random.choice([0, 1])\n",
        "      encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
        "          data_set, bucket_id)\n",
        "      model.step(sess, encoder_inputs, decoder_inputs, target_weights,\n",
        "                 bucket_id, False)\n",
        "\n",
        "\n",
        "def main(_):\n",
        "  if FLAGS.self_test:\n",
        "    self_test()\n",
        "  elif FLAGS.decode:\n",
        "    decode()\n",
        "  elif FLAGS.test:\n",
        "    from_test_data = os.path.join(FLAGS.data_dir,\"test_q.txt\")\n",
        "    to_test_data = os.path.join(FLAGS.data_dir,\"test_f.txt\")\n",
        "    if not (os.path.exists(from_test_data) and os.path.exists(to_test_data)):\n",
        "        if FLAGS.test_file:\n",
        "            data_utils.splitToFrom(FLAGS.data_dir,FLAGS.test_file,\"test\") # split to-from data\n",
        "        else:\n",
        "            print(\"test data file missing!\")\n",
        "            return\n",
        "    \n",
        "    print(\"computing test accuracy...\")\n",
        "    test_acc = test_accuracy(from_test_data,to_test_data)\n",
        "    print(\"test accuracy =\",test_acc)\n",
        "  else:\n",
        "    train()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  tf.app.run()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVgcGWTdKTva",
        "colab_type": "code",
        "outputId": "a8310246-1113-45ee-9187-3c3aab769fcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1856
        }
      },
      "cell_type": "code",
      "source": [
        "!python model/parse_s2s_att.py --data_dir=data --train_dir=checkpoint --train_file=geoqueries_train.txt --test_file=geoqueries_test.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "using training files data/train_q.txt and data/train_f.txt\n",
            "to-from split complete. number of lines = 600\n",
            "to-from split complete. number of lines = 280\n",
            "using test files data/test_q.txt and data/test_f.txt for validation\n",
            "creating vocab from data/train_f.txt\n",
            "Creating vocabulary data/vocab2000.to from data data/train_f.txt\n",
            "creating vocab from data/train_q.txt\n",
            "Creating vocabulary data/vocab2000.from from data data/train_q.txt\n",
            "tokenizing file data/train_f.txt\n",
            "Tokenizing data in data/train_f.txt\n",
            "tokenizing file data/train_q.txt\n",
            "Tokenizing data in data/train_q.txt\n",
            "tokenizing file data/test_f.txt\n",
            "Tokenizing data in data/test_f.txt\n",
            "tokenizing file data/test_q.txt\n",
            "Tokenizing data in data/test_q.txt\n",
            "data preparation complete!\n",
            "2019-04-02 22:40:13.150918: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-02 22:40:13.151572: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1a231e0 executing computations on platform Host. Devices:\n",
            "2019-04-02 22:40:13.151660: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-02 22:40:13.285746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-02 22:40:13.286327: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x1a22b00 executing computations on platform CUDA. Devices:\n",
            "2019-04-02 22:40:13.286374: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-04-02 22:40:13.286925: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-04-02 22:40:13.286962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-02 22:40:14.782081: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-02 22:40:14.782181: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-02 22:40:14.782205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-02 22:40:14.782808: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Creating 1 layers of 200 units.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/parse_seq2seq/model/seq2seq_model.py:124: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "2019-04-02 22:40:18.092741: W tensorflow/python/util/util.cc:302] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py:863: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:1259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Created model with fresh parameters.\n",
            "Reading development and training data (limit: 0).\n",
            "train_bucket_sizes = [600]\n",
            "train_total_size = 600.0\n",
            "generating batches...\n",
            "------------------------------------------\n",
            "Starting training: max epochs = 90 ; epoch size = 30 ; train_total_size = 600.0\n",
            "------------------------------------------\n",
            "2019-04-02 22:42:20.268918: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "epoch 4/90 step 100 learning rate 0.010000 time/batch 1.38 loss 4.07812\n",
            "epoch 7/90 step 200 learning rate 0.009604 time/batch 0.83 loss 2.56847\n",
            "epoch 10/90 step 300 learning rate 0.009039 time/batch 0.82 loss 2.20580\n",
            "  epoch 10 eval: bucket 0 perplexity 2.43 loss 0.88600\n",
            "epoch 14/90 step 400 learning rate 0.008337 time/batch 0.87 loss 0.70056\n",
            "epoch 17/90 step 500 learning rate 0.007847 time/batch 0.85 loss 0.32064\n",
            "epoch 20/90 step 600 learning rate 0.007386 time/batch 0.83 loss 0.18494\n",
            "  epoch 20 eval: bucket 0 perplexity 1.24 loss 0.21156\n",
            "epoch 24/90 step 700 learning rate 0.006812 time/batch 0.84 loss 0.14045\n",
            "epoch 27/90 step 800 learning rate 0.006412 time/batch 0.84 loss 0.09966\n",
            "epoch 30/90 step 900 learning rate 0.006035 time/batch 0.83 loss 0.08126\n",
            "  epoch 30 eval: bucket 0 perplexity 1.17 loss 0.15824\n",
            "epoch 34/90 step 1000 learning rate 0.005566 time/batch 0.84 loss 0.07323\n",
            "epoch 37/90 step 1100 learning rate 0.005239 time/batch 0.81 loss 0.05624\n",
            "epoch 40/90 step 1200 learning rate 0.004931 time/batch 0.82 loss 0.05027\n",
            "  epoch 40 eval: bucket 0 perplexity 1.22 loss 0.19823\n",
            "epoch 44/90 step 1300 learning rate 0.004548 time/batch 0.81 loss 0.04153\n",
            "epoch 47/90 step 1400 learning rate 0.004281 time/batch 0.81 loss 0.03285\n",
            "epoch 50/90 step 1500 learning rate 0.004029 time/batch 0.83 loss 0.02981\n",
            "  epoch 50 eval: bucket 0 perplexity 1.15 loss 0.13768\n",
            "epoch 54/90 step 1600 learning rate 0.003716 time/batch 0.84 loss 0.02444\n",
            "epoch 57/90 step 1700 learning rate 0.003497 time/batch 0.82 loss 0.02226\n",
            "epoch 60/90 step 1800 learning rate 0.003292 time/batch 0.81 loss 0.01762\n",
            "  epoch 60 eval: bucket 0 perplexity 1.11 loss 0.10276\n",
            "epoch 64/90 step 1900 learning rate 0.003036 time/batch 0.85 loss 0.01631\n",
            "epoch 67/90 step 2000 learning rate 0.002858 time/batch 0.82 loss 0.01518\n",
            "epoch 70/90 step 2100 learning rate 0.002690 time/batch 0.81 loss 0.01231\n",
            "  epoch 70 eval: bucket 0 perplexity 1.03 loss 0.02856\n",
            "epoch 74/90 step 2200 learning rate 0.002481 time/batch 0.81 loss 0.01219\n",
            "epoch 77/90 step 2300 learning rate 0.002335 time/batch 0.82 loss 0.00719\n",
            "epoch 80/90 step 2400 learning rate 0.002198 time/batch 0.82 loss 0.00829\n",
            "  epoch 80 eval: bucket 0 perplexity 1.11 loss 0.10777\n",
            "epoch 84/90 step 2500 learning rate 0.002027 time/batch 0.82 loss 0.00716\n",
            "epoch 87/90 step 2600 learning rate 0.001908 time/batch 0.80 loss 0.00579\n",
            "epoch 90/90 step 2700 learning rate 0.001796 time/batch 0.83 loss 0.00526\n",
            "  epoch 90 eval: bucket 0 perplexity 1.60 loss 0.46928\n",
            "Max epoch reached!\n",
            "Model saved.\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "D2K5JZ3ZKZfy",
        "colab_type": "code",
        "outputId": "2276dee2-bbfe-43fe-d038-280ad2fef05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        }
      },
      "cell_type": "code",
      "source": [
        "!python model/parse_s2s_att.py --data_dir=data --train_dir=checkpoint --test_file=geoqueries_test.txt --test=True\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "computing test accuracy...\n",
            "2019-04-03 00:57:33.024243: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-03 00:57:33.024822: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x22031e0 executing computations on platform Host. Devices:\n",
            "2019-04-03 00:57:33.024879: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-03 00:57:33.128690: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-03 00:57:33.129248: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2202b00 executing computations on platform CUDA. Devices:\n",
            "2019-04-03 00:57:33.129286: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-04-03 00:57:33.129949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-04-03 00:57:33.129988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-03 00:57:33.531604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-03 00:57:33.531667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-03 00:57:33.531686: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-03 00:57:33.532133: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/parse_seq2seq/model/seq2seq_model.py:124: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py:863: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From model/parse_s2s_att.py:139: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Reading model parameters from checkpoint/parse.ckpt-2700\n",
            "loading data...\n",
            "data loaded. computing accuracy...\n",
            "2019-04-03 00:58:07.415474: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "test accuracy = 0.8071428571428572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "omyRK9ezs1t9",
        "colab_type": "code",
        "outputId": "0dce8907-251b-448a-db38-24640e0207e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1329
        }
      },
      "cell_type": "code",
      "source": [
        "!python model/parse_s2s_att.py --data_dir=data --train_dir=checkpoint --decode=True"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-04-03 01:01:13.026339: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2019-04-03 01:01:13.026712: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2dbb1e0 executing computations on platform Host. Devices:\n",
            "2019-04-03 01:01:13.026768: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined>\n",
            "2019-04-03 01:01:13.132259: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:998] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2019-04-03 01:01:13.132860: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x2dbab00 executing computations on platform CUDA. Devices:\n",
            "2019-04-03 01:01:13.132903: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): Tesla K80, Compute Capability 3.7\n",
            "2019-04-03 01:01:13.133325: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1433] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.10GiB\n",
            "2019-04-03 01:01:13.133364: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1512] Adding visible gpu devices: 0\n",
            "2019-04-03 01:01:13.541923: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2019-04-03 01:01:13.541984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:990]      0 \n",
            "2019-04-03 01:01:13.542005: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1003] 0:   N \n",
            "2019-04-03 01:01:13.542336: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10754 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "\n",
            "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From /content/parse_seq2seq/model/seq2seq_model.py:124: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/contrib/legacy_seq2seq/python/ops/seq2seq.py:863: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:1444: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "WARNING:tensorflow:From model/parse_s2s_att.py:139: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "Reading model parameters from checkpoint/parse.ckpt-2700\n",
            "> where is boston?\n",
            "[47, 7, 3]\n",
            "2019-04-03 01:03:15.024101: I tensorflow/stream_executor/dso_loader.cc:152] successfully opened CUDA library libcublas.so.10.0 locally\n",
            "( lambda $0 ( loc:<> m0 $0 ) )\n",
            "> is boston in USA?\n",
            "[7, 3, 8, 3]\n",
            "( elevation:<> m0 )\n",
            "> where is the smallest city\n",
            "[47, 7, 4, 35, 29]\n",
            "( lambda $0 ( loc:<> ( argmin:<> ( lambda $1 ( and:<> ( city:<> $1 ) ( loc:<> $1 co0 ) ) ) ( lambda $2 ( size:<> $2 ) ) ) $0 ) )\n",
            "> which state is the smallest?\n",
            "[16, 9, 7, 4, 3]\n",
            "( lambda $0 ( and:<> ( state:<> $0 ) ( loc:<> ( argmax:<> ( lambda $1 ( state:<> $1 ) ) ( lambda $2 ( elevation:<> $2 ) ) ) $0 ) ) )\n",
            "> Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/contextlib.py\", line 99, in __exit__\n",
            "    self.gen.throw(type, value, traceback)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\", line 5253, in get_controller\n",
            "    yield g\n",
            "  File \"model/parse_s2s_att.py\", line 477, in decode\n",
            "    sentence = sys.stdin.readline()\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"model/parse_s2s_att.py\", line 525, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/platform/app.py\", line 125, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"model/parse_s2s_att.py\", line 507, in main\n",
            "    decode()\n",
            "  File \"model/parse_s2s_att.py\", line 477, in decode\n",
            "    sentence = sys.stdin.readline()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 1592, in __exit__\n",
            "    self.close()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 728, in close\n",
            "    tf_session.TF_CloseSession(self._session)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "iAUlNKDOtrpr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}